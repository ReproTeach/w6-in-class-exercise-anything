{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6130c2-3a8c-4401-ae9b-c8cc743a074b",
   "metadata": {},
   "source": [
    "# Week 4 Exercise (group): Exploratory Data Analysis on Social Media Data\n",
    "\n",
    "- Lori Lou\n",
    "- Nayeon Bae\n",
    "- fai slavianto \n",
    "- Ruiya Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a40a3-9ea2-4d3f-a8c5-2d98336ec4bc",
   "metadata": {},
   "source": [
    "## 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8308e587-737b-44aa-b3ee-de8542e973f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04965d8-4ffb-48db-8408-e6d1f361b7be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Using cached emoji-2.2.0-py3-none-any.whl\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283381c6-1485-4633-b127-d7152f189f27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4a085-74e8-4093-85e4-9a2f8008bf04",
   "metadata": {},
   "source": [
    "post_text## 2. Read the data\n",
    "\n",
    "The data is called `tweets.csv` in the same folder. More information about the data see [here](https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media)\n",
    "\n",
    "The main column you will be working with is `post_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c10a6b2a-52c4-4409-ba19-f199bff034b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_created</th>\n",
       "      <th>post_text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>favourites</th>\n",
       "      <th>statuses</th>\n",
       "      <th>retweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>637894677824413696</td>\n",
       "      <td>Sun Aug 30 07:48:37 +0000 2015</td>\n",
       "      <td>It's just over 2 years since I was diagnosed w...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>637890384576778240</td>\n",
       "      <td>Sun Aug 30 07:31:33 +0000 2015</td>\n",
       "      <td>It's Sunday, I need a break, so I'm planning t...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>637749345908051968</td>\n",
       "      <td>Sat Aug 29 22:11:07 +0000 2015</td>\n",
       "      <td>Awake but tired. I need to sleep but my brain ...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>637696421077123073</td>\n",
       "      <td>Sat Aug 29 18:40:49 +0000 2015</td>\n",
       "      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>637696327485366272</td>\n",
       "      <td>Sat Aug 29 18:40:26 +0000 2015</td>\n",
       "      <td>It’s hard to say whether packing lists are mak...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>19995</td>\n",
       "      <td>819336825231773698</td>\n",
       "      <td>Thu Jan 12 00:14:56 +0000 2017</td>\n",
       "      <td>A day without sunshine is like night.</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>19996</td>\n",
       "      <td>819334654260080640</td>\n",
       "      <td>Thu Jan 12 00:06:18 +0000 2017</td>\n",
       "      <td>Boren's Laws: (1) When in charge, ponder. (2) ...</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19997</td>\n",
       "      <td>819334503042871297</td>\n",
       "      <td>Thu Jan 12 00:05:42 +0000 2017</td>\n",
       "      <td>The flow chart is a most thoroughly oversold p...</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>19998</td>\n",
       "      <td>819334419374899200</td>\n",
       "      <td>Thu Jan 12 00:05:22 +0000 2017</td>\n",
       "      <td>Ships are safe in harbor, but they were never ...</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>19999</td>\n",
       "      <td>819334270825197568</td>\n",
       "      <td>Thu Jan 12 00:04:47 +0000 2017</td>\n",
       "      <td>Black holes are where God is dividing by zero.</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0             post_id                    post_created  \\\n",
       "0               0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n",
       "1               1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n",
       "2               2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n",
       "3               3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n",
       "4               4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n",
       "...           ...                 ...                             ...   \n",
       "19995       19995  819336825231773698  Thu Jan 12 00:14:56 +0000 2017   \n",
       "19996       19996  819334654260080640  Thu Jan 12 00:06:18 +0000 2017   \n",
       "19997       19997  819334503042871297  Thu Jan 12 00:05:42 +0000 2017   \n",
       "19998       19998  819334419374899200  Thu Jan 12 00:05:22 +0000 2017   \n",
       "19999       19999  819334270825197568  Thu Jan 12 00:04:47 +0000 2017   \n",
       "\n",
       "                                               post_text     user_id  \\\n",
       "0      It's just over 2 years since I was diagnosed w...  1013187241   \n",
       "1      It's Sunday, I need a break, so I'm planning t...  1013187241   \n",
       "2      Awake but tired. I need to sleep but my brain ...  1013187241   \n",
       "3      RT @SewHQ: #Retro bears make perfect gifts and...  1013187241   \n",
       "4      It’s hard to say whether packing lists are mak...  1013187241   \n",
       "...                                                  ...         ...   \n",
       "19995              A day without sunshine is like night.  1169875706   \n",
       "19996  Boren's Laws: (1) When in charge, ponder. (2) ...  1169875706   \n",
       "19997  The flow chart is a most thoroughly oversold p...  1169875706   \n",
       "19998  Ships are safe in harbor, but they were never ...  1169875706   \n",
       "19999     Black holes are where God is dividing by zero.  1169875706   \n",
       "\n",
       "       followers  friends  favourites  statuses  retweets  label  \n",
       "0             84      211         251       837         0      1  \n",
       "1             84      211         251       837         1      1  \n",
       "2             84      211         251       837         0      1  \n",
       "3             84      211         251       837         2      1  \n",
       "4             84      211         251       837         1      1  \n",
       "...          ...      ...         ...       ...       ...    ...  \n",
       "19995        442      230           7   1063601         0      0  \n",
       "19996        442      230           7   1063601         0      0  \n",
       "19997        442      230           7   1063601         0      0  \n",
       "19998        442      230           7   1063601         0      0  \n",
       "19999        442      230           7   1063601         0      0  \n",
       "\n",
       "[20000 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets.csv\")\n",
    "df\n",
    "# explore the data characteristic using `df.describe()` or `df.info()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13566cfc-ca98-4a22-89a0-c0774944d289",
   "metadata": {},
   "source": [
    "## 3. Extract emojis\n",
    "\n",
    "Use `emoji` package to extract emojis and put them into a new column called `emojis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d0be0be-6a1f-4e42-a90b-ff7656a3e201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d03e40d-0310-4654-9100-26f4442c17d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        []\n",
       "1        []\n",
       "2        []\n",
       "3        []\n",
       "4        []\n",
       "         ..\n",
       "19995    []\n",
       "19996    []\n",
       "19997    []\n",
       "19998    []\n",
       "19999    []\n",
       "Name: post_text, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function\n",
    "emojis = df['post_text'].apply(extract_emojis)\n",
    "emojis\n",
    "# apply the function to your dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d961f7-b745-4f4d-b516-68d9ef48dce2",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning using Regular Expressions \n",
    "\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Remove hashtags\n",
    "4. Remove special characters\n",
    "5. Remove extra space\n",
    "\n",
    "Code can be found in [week 6 lecture 1](https://github.com/yibeichan/COMM160DS/blob/main/week_6/lecture_part1.ipynb) section `4.4 All-in-One`\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec34e8bc-47ab-4bc9-a623-9436ecf6014b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text) # remove hashtag\n",
    "    text = re.sub(r'\\W', ' ', text)  # remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra spaces\n",
    "    return text.strip()\n",
    "df['new'] = df['post_text'].apply(clean_text)\n",
    "# apply the function to your dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44d9f5-2311-4379-ba10-bd9a4ba87c8e",
   "metadata": {},
   "source": [
    "## 5. Collocation Analysis\n",
    "\n",
    "Choose one analysis from (1)Sentiment Analysis, (2)N-grams and Phrase Analysis, (3)Collocation Analysis, (4)Part-of-Speech Tagging, (5)Named Entity Recognition, and (6)Dependency Parsing.\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28dc46ec-f793-4c16-8720-281772d8d3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb995d2-845a-429e-ad81-b49911914de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "def find_collocations(text, num_collocations=10):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    # Apply a frequency filter to keep only bigrams that appear at least twice\n",
    "    finder.apply_freq_filter(2)\n",
    "    # Apply a word filter to exclude bigrams containing stopwords or punctuations\n",
    "    finder.apply_word_filter(lambda w: w.lower() in stopwords.words('english') or w in string.punctuation)\n",
    "    # Return the top num_collocations bigrams with the highest PMI scores\n",
    "    return finder.nbest(bigram_measures.pmi, num_collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c889f6a-e385-4e61-9f92-ede52547fbc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2899380874.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    collocations = find_collocations.(new)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "collocations = find_collocations.(new)\n",
    "collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6ee3d-825a-4ef0-9193-8d69e95b29ad",
   "metadata": {},
   "source": [
    "## 6. N-grams and Phrase Analysis\n",
    "\n",
    "Choose another analysis from (1)Sentiment Analysis, (2)N-grams and Phrase Analysis, (3)Collocation Analysis, (4)Part-of-Speech Tagging, (5)Named Entity Recognition, and (6)Dependency Parsing.\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f15c89b2-634b-4526-a67f-26806ca3aeba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "215884df-b0f3-4f22-8dda-4452815d63b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "871e9e00-5947-48fe-b236-fc02d29f6937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(It's, just), (just, over), (over, 2), (2, ye...\n",
       "1        [(It's, Sunday,), (Sunday,, I), (I, need), (ne...\n",
       "2        [(Awake, but), (but, tired.), (tired., I), (I,...\n",
       "3        [(RT, @SewHQ:), (@SewHQ:, #Retro), (#Retro, be...\n",
       "4        [(It’s, hard), (hard, to), (to, say), (say, wh...\n",
       "                               ...                        \n",
       "19995    [(A, day), (day, without), (without, sunshine)...\n",
       "19996    [(Boren's, Laws:), (Laws:, (1)), ((1), When), ...\n",
       "19997    [(The, flow), (flow, chart), (chart, is), (is,...\n",
       "19998    [(Ships, are), (are, safe), (safe, in), (in, h...\n",
       "19999    [(Black, holes), (holes, are), (are, where), (...\n",
       "Name: n_grams, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['n_grams'] = df['post_text'].apply(lambda x: generate_ngrams(x, n=2))\n",
    "df['n_grams']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2adeba-0c61-4216-9042-e79337da575c",
   "metadata": {},
   "source": [
    "## 7. Push Your Results to GitHub\n",
    "\n",
    "As you did in previous weeks:\n",
    "1. `git status`\n",
    "2. `git add`\n",
    "3. `git commit -m \"type your message here\"`\n",
    "4. `git push`\n",
    "\n",
    "If you can't push it to GitHub, it's okay to manually uploaded it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
