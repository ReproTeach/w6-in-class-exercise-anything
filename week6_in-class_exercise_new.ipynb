{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6130c2-3a8c-4401-ae9b-c8cc743a074b",
   "metadata": {},
   "source": [
    "# Week6 Exercise (group): Exploratory Data Analysis on Social Media Data\n",
    "\n",
    "- Lori Lou\n",
    "- Nayeon Bae\n",
    "- Fai slavianto\n",
    "- Ruiya Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a40a3-9ea2-4d3f-a8c5-2d98336ec4bc",
   "metadata": {},
   "source": [
    "## 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8308e587-737b-44aa-b3ee-de8542e973f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: textblob in /opt/conda/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2023.5.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.1->textblob) (8.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install nltk\n",
    "!pip install textblob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4a085-74e8-4093-85e4-9a2f8008bf04",
   "metadata": {},
   "source": [
    "## 2. Read the data\n",
    "\n",
    "The data is called `tweets.csv` in the same folder. More information about the data see [here](https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media)\n",
    "\n",
    "The main column you will be working with is `post_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c10a6b2a-52c4-4409-ba19-f199bff034b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_created</th>\n",
       "      <th>post_text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>favourites</th>\n",
       "      <th>statuses</th>\n",
       "      <th>retweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>637894677824413696</td>\n",
       "      <td>Sun Aug 30 07:48:37 +0000 2015</td>\n",
       "      <td>It's just over 2 years since I was diagnosed w...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>637890384576778240</td>\n",
       "      <td>Sun Aug 30 07:31:33 +0000 2015</td>\n",
       "      <td>It's Sunday, I need a break, so I'm planning t...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>637749345908051968</td>\n",
       "      <td>Sat Aug 29 22:11:07 +0000 2015</td>\n",
       "      <td>Awake but tired. I need to sleep but my brain ...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>637696421077123073</td>\n",
       "      <td>Sat Aug 29 18:40:49 +0000 2015</td>\n",
       "      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>637696327485366272</td>\n",
       "      <td>Sat Aug 29 18:40:26 +0000 2015</td>\n",
       "      <td>It‚Äôs hard to say whether packing lists are mak...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>19995</td>\n",
       "      <td>819336825231773698</td>\n",
       "      <td>Thu Jan 12 00:14:56 +0000 2017</td>\n",
       "      <td>A day without sunshine is like night.</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>19996</td>\n",
       "      <td>819334654260080640</td>\n",
       "      <td>Thu Jan 12 00:06:18 +0000 2017</td>\n",
       "      <td>Boren's Laws: (1) When in charge, ponder. (2) ...</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19997</td>\n",
       "      <td>819334503042871297</td>\n",
       "      <td>Thu Jan 12 00:05:42 +0000 2017</td>\n",
       "      <td>The flow chart is a most thoroughly oversold p...</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>19998</td>\n",
       "      <td>819334419374899200</td>\n",
       "      <td>Thu Jan 12 00:05:22 +0000 2017</td>\n",
       "      <td>Ships are safe in harbor, but they were never ...</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>19999</td>\n",
       "      <td>819334270825197568</td>\n",
       "      <td>Thu Jan 12 00:04:47 +0000 2017</td>\n",
       "      <td>Black holes are where God is dividing by zero.</td>\n",
       "      <td>1169875706</td>\n",
       "      <td>442</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>1063601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0             post_id                    post_created  \\\n",
       "0               0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n",
       "1               1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n",
       "2               2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n",
       "3               3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n",
       "4               4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n",
       "...           ...                 ...                             ...   \n",
       "19995       19995  819336825231773698  Thu Jan 12 00:14:56 +0000 2017   \n",
       "19996       19996  819334654260080640  Thu Jan 12 00:06:18 +0000 2017   \n",
       "19997       19997  819334503042871297  Thu Jan 12 00:05:42 +0000 2017   \n",
       "19998       19998  819334419374899200  Thu Jan 12 00:05:22 +0000 2017   \n",
       "19999       19999  819334270825197568  Thu Jan 12 00:04:47 +0000 2017   \n",
       "\n",
       "                                               post_text     user_id  \\\n",
       "0      It's just over 2 years since I was diagnosed w...  1013187241   \n",
       "1      It's Sunday, I need a break, so I'm planning t...  1013187241   \n",
       "2      Awake but tired. I need to sleep but my brain ...  1013187241   \n",
       "3      RT @SewHQ: #Retro bears make perfect gifts and...  1013187241   \n",
       "4      It‚Äôs hard to say whether packing lists are mak...  1013187241   \n",
       "...                                                  ...         ...   \n",
       "19995              A day without sunshine is like night.  1169875706   \n",
       "19996  Boren's Laws: (1) When in charge, ponder. (2) ...  1169875706   \n",
       "19997  The flow chart is a most thoroughly oversold p...  1169875706   \n",
       "19998  Ships are safe in harbor, but they were never ...  1169875706   \n",
       "19999     Black holes are where God is dividing by zero.  1169875706   \n",
       "\n",
       "       followers  friends  favourites  statuses  retweets  label  \n",
       "0             84      211         251       837         0      1  \n",
       "1             84      211         251       837         1      1  \n",
       "2             84      211         251       837         0      1  \n",
       "3             84      211         251       837         2      1  \n",
       "4             84      211         251       837         1      1  \n",
       "...          ...      ...         ...       ...       ...    ...  \n",
       "19995        442      230           7   1063601         0      0  \n",
       "19996        442      230           7   1063601         0      0  \n",
       "19997        442      230           7   1063601         0      0  \n",
       "19998        442      230           7   1063601         0      0  \n",
       "19999        442      230           7   1063601         0      0  \n",
       "\n",
       "[20000 rows x 11 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b261e1d5-a59b-4505-ad5a-b7af8e17c70d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     It's just over 2 years since I was diagnosed w...\n",
      "1     It's Sunday, I need a break, so I'm planning t...\n",
      "2     Awake but tired. I need to sleep but my brain ...\n",
      "3     RT @SewHQ: #Retro bears make perfect gifts and...\n",
      "4     It‚Äôs hard to say whether packing lists are mak...\n",
      "5     Making packing lists is my new hobby... #movin...\n",
      "6     At what point does keeping stuff for nostalgic...\n",
      "7     Currently in the finding-boxes-of-random-shit ...\n",
      "8     Can't be bothered to cook, take away on the wa...\n",
      "9     RT @itventsnews: ITV releases promo video for ...\n",
      "10    ... also, I have too much stuff. Way, way too ...\n",
      "11    I never want to put one of these together agai...\n",
      "12    Moving stuff is bloomin‚Äô knackering... and the...\n",
      "13    Back at the house, moving stuff. It‚Äôs so peace...\n",
      "14    Urgh. Anxiety. FFS where does it come from?! (...\n",
      "15    I have too much stuff. Way, way too much... Ma...\n",
      "16    Hideous traffic on the A14. Must remember to p...\n",
      "17                  Packing and purging. Feels good üòäüëçüèº\n",
      "18    In B&amp;Q looking at internal doors. Fun times üòâ\n",
      "19    Time to get up. So many things to do, such a b...\n",
      "20    It's 6:20... do I get up or lie here a little ...\n",
      "21    There's nothing like cocktails and exhaustion ...\n",
      "22    Great night out with my favourite ladies. Much...\n",
      "23    Sat down on the sofa for a quick rest... an ho...\n",
      "24               How much do I want pizza right now...?\n",
      "25    I will always love you.\\r\\n\\r\\nPeter Gabriel -...\n",
      "26    Such a busy day ahead. I need to focus but I'm...\n",
      "27    Even now the smallest thing still makes my hea...\n",
      "28    Why is choosing a curtain pole suddenly so con...\n",
      "29    Completed on my house. Got the keys. What a fa...\n",
      "30    RT @CPHighWycombe: @CatsProtection Hi - could ...\n",
      "Name: post_text, dtype: object\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     2\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    1\n",
      "13    3\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    2\n",
      "18    1\n",
      "19    0\n",
      "20    1\n",
      "21    0\n",
      "22    1\n",
      "23    0\n",
      "24    0\n",
      "25    0\n",
      "26    0\n",
      "27    0\n",
      "28    0\n",
      "29    3\n",
      "30    0\n",
      "Name: emoji_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print the first 31 row and perform an emoji count\n",
    "print(df['post_text'].head(31))\n",
    "df['emoji_count'] = df['post_text'].apply(emoji.emoji_count)\n",
    "print(df['emoji_count'].head(31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13566cfc-ca98-4a22-89a0-c0774944d289",
   "metadata": {},
   "source": [
    "## 3. Extract emojis\n",
    "\n",
    "Use `emoji` package to extract emojis and put them into a new column called `emojis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2d03e40d-0310-4654-9100-26f4442c17d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         []\n",
      "1         []\n",
      "2         []\n",
      "3         []\n",
      "4         []\n",
      "5         []\n",
      "6         []\n",
      "7         []\n",
      "8      [üòÅüëçüèº]\n",
      "9         []\n",
      "10        []\n",
      "11        []\n",
      "12       [üò•]\n",
      "13    [üè°, üòä]\n",
      "14        []\n",
      "15        []\n",
      "16        []\n",
      "17     [üòäüëçüèº]\n",
      "18       [üòâ]\n",
      "19        []\n",
      "20        []\n",
      "21        []\n",
      "22       [üíï]\n",
      "23        []\n",
      "24        []\n",
      "25        []\n",
      "26        []\n",
      "27        []\n",
      "28        []\n",
      "29     [üè°üéâüíï]\n",
      "30        []\n",
      "Name: new_emojis, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "df['new_emojis'] = df['post_text'].apply(extract_emojis)\n",
    "print(df['new_emojis'].head(31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d961f7-b745-4f4d-b516-68d9ef48dce2",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning using Regular Expressions \n",
    "\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Remove hashtags\n",
    "4. Remove special characters\n",
    "5. Remove extra space\n",
    "\n",
    "Code can be found in [week 6 lecture 1](https://github.com/yibeichan/COMM160DS/blob/main/week_6/lecture_part1.ipynb) section `4.4 All-in-One`\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec34e8bc-47ab-4bc9-a623-9436ecf6014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packing and purging Feels good\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # remove hashtags\n",
    "    text = re.sub(r'\\W', ' ', text)  # remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "\n",
    "# Choose any text from the 'post_text' column\n",
    "text = df['post_text'].iloc[17]  # Replace 0 with the index of the desired text\n",
    "\n",
    "# Clean the chosen text\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44d9f5-2311-4379-ba10-bd9a4ba87c8e",
   "metadata": {},
   "source": [
    "## 5. Analysis 1 (Rename the title with your chosen analysis)\n",
    "\n",
    "Choose one analysis from (1)Sentiment Analysis \n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3bb995d2-845a-429e-ad81-b49911914de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 sentiment: positive\n",
      "Text 2 sentiment: negative\n",
      "Text 3 sentiment: negative\n",
      "Text 4 sentiment: positive\n",
      "Text 5 sentiment: negative\n",
      "Text 6 sentiment: positive\n",
      "Text 7 sentiment: negative\n",
      "Text 8 sentiment: negative\n",
      "Text 9 sentiment: neutral\n",
      "Text 10 sentiment: neutral\n",
      "Text 11 sentiment: positive\n",
      "Text 12 sentiment: neutral\n",
      "Text 13 sentiment: neutral\n",
      "Text 14 sentiment: positive\n",
      "Text 15 sentiment: neutral\n",
      "Text 16 sentiment: positive\n",
      "Text 17 sentiment: neutral\n",
      "Text 18 sentiment: positive\n",
      "Text 19 sentiment: positive\n",
      "Text 20 sentiment: positive\n",
      "Text 21 sentiment: negative\n",
      "Text 22 sentiment: positive\n",
      "Text 23 sentiment: positive\n",
      "Text 24 sentiment: positive\n",
      "Text 25 sentiment: positive\n",
      "Text 26 sentiment: positive\n",
      "Text 27 sentiment: positive\n",
      "Text 28 sentiment: neutral\n",
      "Text 29 sentiment: positive\n",
      "Text 30 sentiment: neutral\n",
      "Text 31 sentiment: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "sentiments = []\n",
    "\n",
    "# Iterate through all 31 locations in the 'post_text' column\n",
    "for i in range(31):\n",
    "    text = df['post_text'].iloc[i]  # Choose the text from each location\n",
    "    \n",
    "    # Clean the chosen text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    sentiment = get_sentiment(cleaned_text)\n",
    "    \n",
    "    sentiments.append(sentiment)  # Append the sentiment to the list\n",
    "\n",
    "# Print the sentiments for all 31 texts\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    print(f\"Text {i+1} sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6ee3d-825a-4ef0-9193-8d69e95b29ad",
   "metadata": {},
   "source": [
    "## 6. Analysis 2 (Rename the title with your chosen analysis)\n",
    "\n",
    "(4)Part-of-Speech Tagging\n",
    "\n",
    "Perform the analysis and save the results into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f15c89b2-634b-4526-a67f-26806ca3aeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 POS tags breakdown:\n",
      "('It', 'PRP'): It is a PRP.\n",
      "('s', 'VBZ'): s is a NN.\n",
      "('just', 'RB'): just is a RB.\n",
      "('over', 'IN'): over is a IN.\n",
      "('2', 'CD'): 2 is a CD.\n",
      "('years', 'NNS'): years is a NNS.\n",
      "('since', 'IN'): since is a IN.\n",
      "('I', 'PRP'): I is a PRP.\n",
      "('was', 'VBD'): was is a VBD.\n",
      "('diagnosed', 'VBN'): diagnosed is a VBN.\n",
      "('with', 'IN'): with is a IN.\n",
      "('and', 'CC'): and is a CC.\n",
      "('Today', 'NNP'): Today is a NN.\n",
      "('I', 'PRP'): I is a PRP.\n",
      "('m', 'VBP'): m is a NN.\n",
      "('taking', 'VBG'): taking is a VBG.\n",
      "('a', 'DT'): a is a DT.\n",
      "('moment', 'NN'): moment is a NN.\n",
      "('to', 'TO'): to is a TO.\n",
      "('reflect', 'VB'): reflect is a NN.\n",
      "('on', 'IN'): on is a IN.\n",
      "('how', 'WRB'): how is a WRB.\n",
      "('far', 'RB'): far is a RB.\n",
      "('I', 'PRP'): I is a PRP.\n",
      "('ve', 'VBP'): ve is a NN.\n",
      "('come', 'VBN'): come is a VB.\n",
      "('since', 'IN'): since is a IN.\n",
      "\n",
      "Text 2 POS tags breakdown:\n",
      "('It', 'PRP'): It is a PRP.\n",
      "('s', 'VBD'): s is a NN.\n",
      "('Sunday', 'NNP'): Sunday is a NNP.\n",
      "('I', 'PRP'): I is a PRP.\n",
      "('need', 'VBP'): need is a NN.\n",
      "('a', 'DT'): a is a DT.\n",
      "('break', 'NN'): break is a NN.\n",
      "('so', 'IN'): so is a RB.\n",
      "('I', 'PRP'): I is a PRP.\n",
      "('m', 'VBP'): m is a NN.\n",
      "('planning', 'VBG'): planning is a NN.\n",
      "('to', 'TO'): to is a TO.\n",
      "('spend', 'VB'): spend is a NN.\n",
      "('as', 'IN'): as is a IN.\n",
      "('little', 'JJ'): little is a JJ.\n",
      "('time', 'NN'): time is a NN.\n",
      "('as', 'IN'): as is a IN.\n",
      "('possible', 'JJ'): possible is a JJ.\n",
      "('on', 'IN'): on is a IN.\n",
      "('the', 'DT'): the is a DT.\n",
      "\n",
      "Text 3 POS tags breakdown:\n",
      "('Awake', 'NNP'): Awake is a VB.\n",
      "('but', 'CC'): but is a CC.\n",
      "('tired', 'VBD'): tired is a VBN.\n",
      "('I', 'PRP'): I is a PRP.\n",
      "('need', 'VBP'): need is a NN.\n",
      "('to', 'TO'): to is a TO.\n",
      "('sleep', 'VB'): sleep is a NN.\n",
      "('but', 'CC'): but is a CC.\n",
      "('my', 'PRP$'): my is a PRP$.\n",
      "('brain', 'NN'): brain is a NN.\n",
      "('has', 'VBZ'): has is a VBZ.\n",
      "('other', 'JJ'): other is a JJ.\n",
      "('ideas', 'NNS'): ideas is a NNS.\n",
      "\n",
      "Text 4 POS tags breakdown:\n",
      "('RT', 'NNP'): RT is a NN.\n",
      "('bears', 'NNS'): bears is a NNS.\n",
      "('make', 'VBP'): make is a VB.\n",
      "('perfect', 'JJ'): perfect is a NN.\n",
      "('gifts', 'NNS'): gifts is a NNS.\n",
      "('and', 'CC'): and is a CC.\n",
      "('are', 'VBP'): are is a VBP.\n",
      "('great', 'JJ'): great is a JJ.\n",
      "('for', 'IN'): for is a IN.\n",
      "('beginners', 'NNS'): beginners is a NNS.\n",
      "('too', 'RB'): too is a RB.\n",
      "('Get', 'NNP'): Get is a VB.\n",
      "('stitching', 'VBG'): stitching is a VBG.\n",
      "('with', 'IN'): with is a IN.\n",
      "('October', 'NNP'): October is a NNP.\n",
      "('s', 'JJ'): s is a NN.\n",
      "('Sew', 'NNP'): Sew is a NN.\n",
      "('on', 'IN'): on is a IN.\n",
      "('sale', 'NN'): sale is a NN.\n",
      "('NOW', 'NN'): NOW is a NN.\n",
      "\n",
      "Text 5 POS tags breakdown:\n",
      "('It', 'PRP'): It is a PRP.\n",
      "('s', 'VBZ'): s is a NN.\n",
      "('hard', 'JJ'): hard is a JJ.\n",
      "('to', 'TO'): to is a TO.\n",
      "('say', 'VB'): say is a VB.\n",
      "('whether', 'IN'): whether is a IN.\n",
      "('packing', 'VBG'): packing is a VBG.\n",
      "('lists', 'NNS'): lists is a NNS.\n",
      "('are', 'VBP'): are is a VBP.\n",
      "('making', 'VBG'): making is a VBG.\n",
      "('life', 'NN'): life is a NN.\n",
      "('easier', 'JJR'): easier is a JJR.\n",
      "('or', 'CC'): or is a CC.\n",
      "('just', 'RB'): just is a RB.\n",
      "('reinforcing', 'VBG'): reinforcing is a VBG.\n",
      "('how', 'WRB'): how is a WRB.\n",
      "('much', 'JJ'): much is a JJ.\n",
      "('still', 'RB'): still is a RB.\n",
      "('needs', 'VBZ'): needs is a NNS.\n",
      "('doing', 'VBG'): doing is a VBG.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tagging(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "\n",
    "# Iterate through all 31 locations in the 'post_text' column\n",
    "for i in range(5):\n",
    "    text = df['post_text'].iloc[i]  # Choose the text from each location\n",
    "    \n",
    "    # Clean the chosen text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tagging(cleaned_text)\n",
    "    \n",
    "    print(f\"Text {i+1} POS tags breakdown:\")\n",
    "    for token, pos in pos_tags:\n",
    "        print(f\"('{token}', '{pos}'): {token} is a {nltk.pos_tag([token])[0][1]}.\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2adeba-0c61-4216-9042-e79337da575c",
   "metadata": {},
   "source": [
    "## 7. Push Your Results to GitHub\n",
    "\n",
    "As you did in previous weeks:\n",
    "1. `git status`\n",
    "2. `git add`\n",
    "3. `git commit -m \"type your message here\"`\n",
    "4. `git push`\n",
    "\n",
    "If you can't push it to GitHub, it's okay to manually uploaded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a5440-f3a5-4e18-bfaf-ec5aaaba91b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
